This guide helps you set up the project locally. It covers both the **FastAPI backend** and **React frontend**.

---

## ğŸ§° Prerequisites

### 1. Python Setup

* Install Python 3.10 or newer:

  * [Download Python](https://www.python.org/downloads/)
  * Make sure to check **"Add Python to PATH"** during installation

### 2. Node.js & npm

* Install Node.js (which includes npm):

  * [Download Node.js LTS](https://nodejs.org/en/)

---

## ğŸ“‚ Folder Structure

```
root/
â”œâ”€â”€ server/            # FastAPI backend
â”‚   â””â”€â”€ main.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ client/            # React frontend (Vite)
â”‚   â””â”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
```

---

## ğŸš€ Backend Setup (FastAPI)

### Create & activate virtual environment for backend

Note: This step is required ONLY during the initial deployment.
```bash
cd server
python -m venv venv 
source venv/bin/activate  # On Windows: venv\Scripts\activate  
pip install -r requirements.txt
```

### Run your backend in the `server/` folder
```bash
uvicorn main:app --reload --port 8000 #Run the backend on port:8000
```

### Create `.env` file

Create a `.env` in the `server/` folder:

```
COHERE_API_KEY=your_actual_key_here
```
Visit: [https://dashboard.cohere.com/api-keys](Cohere API Keys) to get your API key

---

## ğŸ’» Frontend Setup (React + Vite)

### Install dependencies for frontend

Note: This step is required ONLY during the initial deployment.
```bash
cd client
npm install
```

### Run your frontend in the `client/` folder
```bash
npm run dev #Run the frontend at port:5173
```
Visit: [http://localhost:5173](http://localhost:5173)

---

## ğŸš€ How does it work?

This project is a lightweight, server-hosted RAG (Retrieval-Augmented Generation) system that allows users to upload a PDF and ask questions based on its content â€” all powered by vector embeddings and Cohereâ€™s large language model.

### ğŸ§  Step-by-step Breakdown

1. **ğŸ“¤ Upload a PDF**  
   Send a `POST` request to `/upload` with your PDF file.

2. **ğŸ“„ Text Extraction**  
   The PDF is processed using `PyMuPDF`, which extracts clean, raw text from each page.

3. **ğŸ”ª Text Chunking**  
   The extracted text is chunked using section headings like `Definition`, `Theorem`, `Example`, etc., via regex-based splitting.

4. **ğŸ§¬ Chunk Embedding**  
   Each chunk is embedded using `SentenceTransformer("all-MiniLM-L6-v2")`, converting it into a **384-dimensional vector**.  
   These vectors are stored in a **FAISS index** for fast similarity search.

5. **â“ Ask a Question**  
   You send a question via a `POST` request to `/context`.  
   The system:
   - Embeds the question using the same model
   - Searches the FAISS index for the **top-k most similar chunks** (default `k=5`)

6. **ğŸ§  Context-Aware Answering**  
   A custom RAG prompt is built using your question and the retrieved chunks.  
   This prompt is sent to **Cohere's LLM**, which generates an answer based only on that context (zero hallucination).

7. **ğŸ’¬ Final Output**  
   You receive an accurate, context-grounded answer â€” just like chatting with your textbook!

---

## ğŸ› ï¸ Upcoming Features

- **ğŸ” Chat History**  
  Previous questions and responses will be saved for a more coherent conversation.

- **ğŸ§  Memory**  
  The AI will remember your previous chats for better follow-ups.

- **ğŸ“ Chunking Flexibility**  
  Customize how the text is chunked â€” sentence-level, paragraph-level, or hybrid.

- **ğŸ“ Custom Prompting**  
  Use your own RAG prompt templates to guide how the AI answers.

- **ğŸ“Š Reranking for Relevance**  
  Retrieved chunks will be reranked using additional signals for more accurate answers.

- **ğŸ–¼ï¸ Image Embedding (Coming Soon)**  
  Upload textbook images â€” the system will extract and embed visual content for image-based QA.

---
